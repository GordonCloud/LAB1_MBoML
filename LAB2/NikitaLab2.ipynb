{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrXL3zjITF5G",
        "outputId": "fb4d389b-381e-4b17-e5ef-898518e5e376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.75013529 0.50007857 0.24997273]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "X = np.random.rand(1000,10)\n",
        "x = []\n",
        "p = []\n",
        "c1 = 1\n",
        "c2 = 1\n",
        "def Function(x):\n",
        "    return 0.5 * ((x[0])**2 + (x[0] - x[1])**2 + (x[1] - x[2])**2 + (x[2])**2) - x[0]\n",
        "\n",
        "def analytic_grad(f, x):\n",
        "    return np.array([2 * x[0] - x[1] - 1, -x[0] + 2 * x[1] - x[2], - x[1] + 2 * x[2]])\n",
        "\n",
        "def num_grad(f, x, h = 10e-4):\n",
        "    return (f(x - h * np.ones(x.shape)) + f(x + h * np.ones(x.shape))) / 2 / h\n",
        "\n",
        "\n",
        "def update_hessian(old_hessian, sk, yk):\n",
        "    ro = 1 / (np.dot(yk.T, sk))\n",
        "    I = np.eye(old_hessian.shape[0])\n",
        "\n",
        "    term1 = I - ro * np.outer(sk, yk)\n",
        "    term2 = I - ro * np.outer(yk, sk)\n",
        "    term3 = ro * np.outer(sk, sk)\n",
        "\n",
        "    new_hessian = np.dot(np.dot(term1, old_hessian), term2) + term3\n",
        "\n",
        "    return new_hessian\n",
        "\n",
        "def zoom(f, grad, x, p, c1, c2, alpha_low, alpha_high):\n",
        "    grad_f = grad(f, x)\n",
        "    while True:\n",
        "        alpha_j = 0.5 * (alpha_low + alpha_high)\n",
        "        f_j = f(x + alpha_j * p)\n",
        "        if (f_j > f(x) + c1 * alpha_j * np.dot(grad_f, p)) or (f_j >= f(x + alpha_low * p)):\n",
        "            alpha_high = alpha_j\n",
        "        else:\n",
        "            grad_f_j = grad(f, x + alpha_j * p)\n",
        "            if np.abs(np.dot(grad_f_j, p)) <= -c2 * np.dot(grad_f, p):\n",
        "                return alpha_j\n",
        "            if np.dot(grad_f_j, p) * (alpha_high - alpha_low) >= 0:\n",
        "                alpha_high = alpha_low\n",
        "            alpha_low = alpha_j\n",
        "\n",
        "\n",
        "def line_search(f, grad, x, p, c1 = 0.01, c2 = 0.04, alpha_max = 1000, maxiter = 1000):\n",
        "    alpha_prev = 0\n",
        "    alpha_cur = (alpha_prev + alpha_max) / 2\n",
        "    grad_f = grad(f, x)\n",
        "    i = 1\n",
        "    while i <= maxiter:\n",
        "        f_i = f(x + alpha_cur * p)\n",
        "        if (f_i > f(x) + c1 * alpha_cur * np.dot(grad_f, p)) or (f_i >= f(x + alpha_prev * p) and i > 1):\n",
        "            return zoom(f, grad, x, p, c1, c2, alpha_prev, alpha_cur)\n",
        "        grad_alpha_f = grad(f, x + alpha_cur * p)\n",
        "        if np.abs(np.dot(grad_alpha_f, p)) <= -c2 * np.dot(grad_f, p):\n",
        "            return alpha_cur\n",
        "        if grad_alpha_f >= 0:\n",
        "            return zoom(f, grad, x, p, c1, c2, alpha_prev, alpha_cur)\n",
        "        alpha_prev = alpha_cur\n",
        "        alpha_cur = (alpha_prev + alpha_max) / 2\n",
        "        i += 1\n",
        "\n",
        "\n",
        "def BFGS(f, x_0, maxiter = 1000, eps = 10e-4, type = \"analytical\"):\n",
        "    count = 0\n",
        "    H_k = np.eye(len(x_0))\n",
        "    x_k = x_0\n",
        "    fgrad = 0\n",
        "    if type == \"analytical\":\n",
        "        fgrad = analytic_grad\n",
        "    elif type == \"numerical\":\n",
        "        fgrad = num_grad\n",
        "\n",
        "    fgrad_value = fgrad(f, x_k)\n",
        "\n",
        "    while np.linalg.norm(fgrad_value) > eps and count < maxiter:\n",
        "\n",
        "        p_k = -np.dot(H_k, fgrad_value)\n",
        "        alpha_k = line_search(f, fgrad, x_k, p_k)\n",
        "        x_next = x_k + alpha_k * p_k\n",
        "        fgrad_value_next = fgrad(f, x_next)\n",
        "        H_k = update_hessian(H_k, x_next - x_k, fgrad_value_next - fgrad_value)\n",
        "        x_k = x_next\n",
        "        fgrad_value = fgrad_value_next\n",
        "\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    return x_k\n",
        "\n",
        "x_final = BFGS(f = Function, x_0 = [1, 1, 1])\n",
        "\n",
        "print (x_final)\n"
      ]
    }
  ]
}